{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "# https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"inputs/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.418062e+06</td>\n",
       "      <td>1.503424e+06</td>\n",
       "      <td>1.390836e+06</td>\n",
       "      <td>1.503424e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.167081e+05</td>\n",
       "      <td>7.436740e+02</td>\n",
       "      <td>1.241932e+03</td>\n",
       "      <td>1.391306e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.689154e+07</td>\n",
       "      <td>5.572522e+03</td>\n",
       "      <td>9.704641e+02</td>\n",
       "      <td>2.600785e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>4.250000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.300000e+03</td>\n",
       "      <td>2.900000e+01</td>\n",
       "      <td>1.057000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000e+03</td>\n",
       "      <td>8.800000e+01</td>\n",
       "      <td>2.217000e+03</td>\n",
       "      <td>1.508700e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.950101e+10</td>\n",
       "      <td>2.044290e+05</td>\n",
       "      <td>3.066000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price  item_seq_number   image_top_1  deal_probability\n",
       "count  1.418062e+06     1.503424e+06  1.390836e+06      1.503424e+06\n",
       "mean   3.167081e+05     7.436740e+02  1.241932e+03      1.391306e-01\n",
       "std    6.689154e+07     5.572522e+03  9.704641e+02      2.600785e-01\n",
       "min    0.000000e+00     1.000000e+00  0.000000e+00      0.000000e+00\n",
       "25%    5.000000e+02     9.000000e+00  4.250000e+02      0.000000e+00\n",
       "50%    1.300000e+03     2.900000e+01  1.057000e+03      0.000000e+00\n",
       "75%    7.000000e+03     8.800000e+01  2.217000e+03      1.508700e-01\n",
       "max    7.950101e+10     2.044290e+05  3.066000e+03      1.000000e+00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>parent_category_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>param_1</th>\n",
       "      <th>param_2</th>\n",
       "      <th>param_3</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>item_seq_number</th>\n",
       "      <th>activation_date</th>\n",
       "      <th>user_type</th>\n",
       "      <th>image</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>deal_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>e00f8ff2eaf9</td>\n",
       "      <td>Свердловская область</td>\n",
       "      <td>Екатеринбург</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Постельные принадлежности</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кокоби(кокон для сна)</td>\n",
       "      <td>Кокон для сна малыша,пользовались меньше месяц...</td>\n",
       "      <td>400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Private</td>\n",
       "      <td>d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>0.12789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>39aeb48f0017</td>\n",
       "      <td>Самарская область</td>\n",
       "      <td>Самара</td>\n",
       "      <td>Для дома и дачи</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>Другое</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Стойка для Одежды</td>\n",
       "      <td>Стойка для одежды, под вешалки. С бутика.</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>Private</td>\n",
       "      <td>79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...</td>\n",
       "      <td>692.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>91e2f88dd6e3</td>\n",
       "      <td>Ростовская область</td>\n",
       "      <td>Ростов-на-Дону</td>\n",
       "      <td>Бытовая электроника</td>\n",
       "      <td>Аудио и видео</td>\n",
       "      <td>Видео, DVD и Blu-ray плееры</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Philips bluray</td>\n",
       "      <td>В хорошем состоянии, домашний кинотеатр с blu ...</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>Private</td>\n",
       "      <td>b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>0.43177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02996f1dd2ea</td>\n",
       "      <td>bf5cccea572d</td>\n",
       "      <td>Татарстан</td>\n",
       "      <td>Набережные Челны</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Товары для детей и игрушки</td>\n",
       "      <td>Автомобильные кресла</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автокресло</td>\n",
       "      <td>Продам кресло от0-25кг</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>286</td>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>Company</td>\n",
       "      <td>e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...</td>\n",
       "      <td>796.0</td>\n",
       "      <td>0.80323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7c90be56d2ab</td>\n",
       "      <td>ef50846afc0b</td>\n",
       "      <td>Волгоградская область</td>\n",
       "      <td>Волгоград</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили</td>\n",
       "      <td>С пробегом</td>\n",
       "      <td>ВАЗ (LADA)</td>\n",
       "      <td>2110</td>\n",
       "      <td>ВАЗ 2110, 2003</td>\n",
       "      <td>Все вопросы по телефону.</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>Private</td>\n",
       "      <td>54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>0.20797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id       user_id                 region              city  \\\n",
       "0  b912c3c6a6ad  e00f8ff2eaf9   Свердловская область      Екатеринбург   \n",
       "1  2dac0150717d  39aeb48f0017      Самарская область            Самара   \n",
       "2  ba83aefab5dc  91e2f88dd6e3     Ростовская область    Ростов-на-Дону   \n",
       "3  02996f1dd2ea  bf5cccea572d              Татарстан  Набережные Челны   \n",
       "4  7c90be56d2ab  ef50846afc0b  Волгоградская область         Волгоград   \n",
       "\n",
       "  parent_category_name               category_name  \\\n",
       "0          Личные вещи  Товары для детей и игрушки   \n",
       "1      Для дома и дачи           Мебель и интерьер   \n",
       "2  Бытовая электроника               Аудио и видео   \n",
       "3          Личные вещи  Товары для детей и игрушки   \n",
       "4            Транспорт                  Автомобили   \n",
       "\n",
       "                       param_1     param_2 param_3                  title  \\\n",
       "0    Постельные принадлежности         NaN     NaN  Кокоби(кокон для сна)   \n",
       "1                       Другое         NaN     NaN      Стойка для Одежды   \n",
       "2  Видео, DVD и Blu-ray плееры         NaN     NaN         Philips bluray   \n",
       "3         Автомобильные кресла         NaN     NaN             Автокресло   \n",
       "4                   С пробегом  ВАЗ (LADA)    2110         ВАЗ 2110, 2003   \n",
       "\n",
       "                                         description    price  \\\n",
       "0  Кокон для сна малыша,пользовались меньше месяц...    400.0   \n",
       "1          Стойка для одежды, под вешалки. С бутика.   3000.0   \n",
       "2  В хорошем состоянии, домашний кинотеатр с blu ...   4000.0   \n",
       "3                             Продам кресло от0-25кг   2200.0   \n",
       "4                           Все вопросы по телефону.  40000.0   \n",
       "\n",
       "   item_seq_number activation_date user_type  \\\n",
       "0                2      2017-03-28   Private   \n",
       "1               19      2017-03-26   Private   \n",
       "2                9      2017-03-20   Private   \n",
       "3              286      2017-03-25   Company   \n",
       "4                3      2017-03-16   Private   \n",
       "\n",
       "                                               image  image_top_1  \\\n",
       "0  d10c7e016e03247a3bf2d13348fe959fe6f436c1caf64c...       1008.0   \n",
       "1  79c9392cc51a9c81c6eb91eceb8e552171db39d7142700...        692.0   \n",
       "2  b7f250ee3f39e1fedd77c141f273703f4a9be59db4b48a...       3032.0   \n",
       "3  e6ef97e0725637ea84e3d203e82dadb43ed3cc0a1c8413...        796.0   \n",
       "4  54a687a3a0fc1d68aed99bdaaf551c5c70b761b16fd0a2...       2264.0   \n",
       "\n",
       "   deal_probability  \n",
       "0           0.12789  \n",
       "1           0.00000  \n",
       "2           0.43177  \n",
       "3           0.80323  \n",
       "4           0.20797  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503424"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'user_id', 'region', 'city', 'parent_category_name',\n",
       "       'category_name', 'param_1', 'param_2', 'param_3', 'title',\n",
       "       'description', 'price', 'item_seq_number', 'activation_date',\n",
       "       'user_type', 'image', 'image_top_1', 'deal_probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177754"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_train[\"deal_probability\"] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1325670"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_train[\"deal_probability\"] <= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"inputs/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508438"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling Missing Values.....\n",
      "Casting data types to type Category.......\n",
      "Creating New Feature.....\n",
      "PreProcessing Function completed.\n",
      "Start Tokenization.....\n",
      "Loading Test for Label Encoding on Train + Test\n",
      "Creating New Feature.....\n",
      "(2011862, 10)\n",
      "Start Label Encoding process....\n",
      "Fit on Train Function completed.\n",
      "Transform done for test\n",
      "Time taken for Sequence Tokens is57.37170124053955\n",
      "Transform on test function completed.\n",
      "Tokenization done and TRAIN READY FOR Validation splitting\n",
      "Train Test Split\n",
      "(1428252, 10) (75172, 10)\n",
      "(1428252,) (75172,)\n",
      "Data ready for Vectorization\n",
      "Data ready for Vectorization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import time \n",
    "import gc \n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from contextlib import closing\n",
    "cores = 4\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "### rmse loss for keras\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Filling Missing Values.....\")\n",
    "    dataset['price'] = dataset['price'].fillna(0).astype('float32')\n",
    "    dataset['param_1'].fillna(value='missing', inplace=True)\n",
    "    dataset['param_2'].fillna(value='missing', inplace=True)\n",
    "    dataset['param_3'].fillna(value='missing', inplace=True)\n",
    "    \n",
    "    dataset['param_1'] = dataset['param_1'].astype(str)\n",
    "    dataset['param_2'] = dataset['param_2'].astype(str)\n",
    "    dataset['param_3'] = dataset['param_3'].astype(str)\n",
    "    \n",
    "    print(\"Casting data types to type Category.......\")\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['parent_category_name'] = dataset['parent_category_name'].astype('category')\n",
    "    dataset['region'] = dataset['region'].astype('category')\n",
    "    dataset['city'] = dataset['city'].astype('category')\n",
    "\n",
    "    print(\"Creating New Feature.....\")\n",
    "    dataset['param123'] = (dataset['param_1']+'_'+dataset['param_2']+'_'+dataset['param_3']).astype(str)\n",
    "    del dataset['param_2'], dataset['param_3']\n",
    "    gc.collect()\n",
    "        \n",
    "    print(\"PreProcessing Function completed.\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def keras_fit(train):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    train['title_description']= (train['title']+\" \"+train['description']).astype(str)\n",
    "    del train['description'], train['title']\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Start Tokenization.....\")\n",
    "    tokenizer = text.Tokenizer(num_words = max_words_title_description)\n",
    "    all_text = np.hstack([train['title_description'].str.lower()])\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    del all_text\n",
    "    del train['activation_date']\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Loading Test for Label Encoding on Train + Test\")\n",
    "    use_cols_test = ['region', 'city', 'parent_category_name', 'category_name', 'param_1', 'param_2', 'param_3']\n",
    "    test = pd.read_csv(\"./inputs/test.csv\", usecols = use_cols_test)\n",
    "    \n",
    "    test['param_1'].fillna(value='missing', inplace=True)\n",
    "    test['param_1'] = test['param_1'].astype(str)\n",
    "    test['param_2'].fillna(value='missing', inplace=True)\n",
    "    test['param_2'] = test['param_2'].astype(str)\n",
    "    test['param_3'].fillna(value='missing', inplace=True)\n",
    "    test['param_3'] = test['param_3'].astype(str)\n",
    "\n",
    "    print(\"Creating New Feature.....\")\n",
    "    test['param123'] = (test['param_1']+'_'+test['param_2']+'_'+test['param_3']).astype(str)\n",
    "    del test['param_2'], test['param_3']\n",
    "    gc.collect()\n",
    "    \n",
    "    ntrain = train.shape[0]\n",
    "    DF = pd.concat([train, test], axis = 0)\n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    print(DF.shape)\n",
    "    \n",
    "    print(\"Start Label Encoding process....\")\n",
    "    le_region = LabelEncoder()\n",
    "    le_region.fit(DF.region)\n",
    "    \n",
    "    le_city = LabelEncoder()\n",
    "    le_city.fit(DF.city)\n",
    "    \n",
    "    le_category_name = LabelEncoder()\n",
    "    le_category_name.fit(DF.category_name)\n",
    "    \n",
    "    le_parent_category_name = LabelEncoder()\n",
    "    le_parent_category_name.fit(DF.parent_category_name)\n",
    "    \n",
    "    le_param_1 = LabelEncoder()\n",
    "    le_param_1.fit(DF.param_1)\n",
    "    \n",
    "    le_param123 = LabelEncoder()\n",
    "    le_param123.fit(DF.param123)\n",
    "\n",
    "    train = DF[0:ntrain]\n",
    "    test = DF[ntrain:]\n",
    "    del DF \n",
    "    gc.collect()\n",
    "    \n",
    "    train['price'] = np.log1p(train['price'])\n",
    "    train['item_seq_number'] = np.log(train['item_seq_number'])\n",
    "    print(\"Fit on Train Function completed.\")\n",
    "    \n",
    "    return train, tokenizer, le_region, le_city, le_category_name, le_parent_category_name, le_param_1, le_param123\n",
    "\n",
    "def keras_train_transform(dataset):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    dataset['seq_title_description']= tokenizer.texts_to_sequences(dataset.title_description.str.lower())\n",
    "    print(\"Transform done for test\")\n",
    "    print(\"Time taken for Sequence Tokens is\"+str(time.time()-t1))\n",
    "    del train['title_description']\n",
    "    gc.collect()\n",
    "\n",
    "    dataset['region'] = le_region.transform(dataset['region'])\n",
    "    dataset['city'] = le_city.transform(dataset['city'])\n",
    "    dataset['category_name'] = le_category_name.transform(dataset['category_name'])\n",
    "    dataset['parent_category_name'] = le_parent_category_name.transform(dataset['parent_category_name'])\n",
    "    dataset['param_1'] = le_param_1.transform(dataset['param_1'])\n",
    "    dataset['param123'] = le_param123.transform(dataset['param123'])\n",
    "    \n",
    "    print(\"Transform on test function completed.\")\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def keras_test_transform(dataset):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    dataset['title_description']= (dataset['title']+\" \"+dataset['description']).astype(str)\n",
    "    del dataset['description'], dataset['title']\n",
    "    gc.collect()\n",
    "    \n",
    "    dataset['seq_title_description']= tokenizer.texts_to_sequences(dataset.title_description.str.lower())\n",
    "    print(\"Transform done for test\")\n",
    "    print(\"Time taken for Sequence Tokens is\"+str(time.time()-t1))\n",
    "    \n",
    "    del dataset['activation_date'], dataset['title_description']\n",
    "    gc.collect()\n",
    "\n",
    "    dataset['region'] = le_region.transform(dataset['region'])\n",
    "    dataset['city'] = le_city.transform(dataset['city'])\n",
    "    dataset['category_name'] = le_category_name.transform(dataset['category_name'])\n",
    "    dataset['parent_category_name'] = le_parent_category_name.transform(dataset['parent_category_name'])\n",
    "    dataset['param_1'] = le_param_1.transform(dataset['param_1'])\n",
    "    dataset['param123'] = le_param123.transform(dataset['param123'])\n",
    "    \n",
    "    dataset['price'] = np.log1p(dataset['price'])\n",
    "    dataset['item_seq_number'] = np.log(dataset['item_seq_number'])\n",
    "    \n",
    "    print(\"Transform on test function completed.\")\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'seq_title_description': pad_sequences(dataset.seq_title_description, maxlen=max_seq_title_description_length)\n",
    "        ,'region': np.array(dataset.region)\n",
    "        ,'city': np.array(dataset.city)\n",
    "        ,'category_name': np.array(dataset.category_name)\n",
    "        ,'parent_category_name': np.array(dataset.parent_category_name)\n",
    "        ,'param_1': np.array(dataset.param_1)\n",
    "        ,'param123': np.array(dataset.param123)\n",
    "        ,'price': np.array(dataset[[\"price\"]])\n",
    "        ,'item_seq_number': np.array(dataset[[\"item_seq_number\"]])\n",
    "    }\n",
    "    \n",
    "    print(\"Data ready for Vectorization\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# Loading Train data - No Params, No Image data \n",
    "dtypes_train = {\n",
    "                'price': 'float32',\n",
    "                'deal probability': 'float32',\n",
    "                'item_seq_number': 'uint32'\n",
    "}\n",
    "\n",
    "# No user_id\n",
    "use_cols = ['item_id', 'region', 'city', 'parent_category_name', 'category_name', 'param_1', 'param_2', 'param_3', 'title', 'description', 'price', 'item_seq_number', 'activation_date', 'deal_probability']\n",
    "train = pd.read_csv(\"./inputs/train.csv\", parse_dates=[\"activation_date\"], usecols = use_cols, dtype = dtypes_train)\n",
    "\n",
    "y_train = train['deal_probability']\n",
    "del train['deal_probability']\n",
    "gc.collect()\n",
    "\n",
    "max_seq_title_description_length = 100\n",
    "max_words_title_description = 200000\n",
    "\n",
    "train = preprocess_dataset(train)\n",
    "train, tokenizer, le_region, le_city, le_category_name, le_parent_category_name, le_param_1, le_param123 = keras_fit(train)\n",
    "train = keras_train_transform(train)\n",
    "print(\"Tokenization done and TRAIN READY FOR Validation splitting\")\n",
    "\n",
    "# Calculation of max values for Categorical fields \n",
    "        \n",
    "max_region = np.max(train.region.max())+1\n",
    "max_city= np.max(train.city.max())+1\n",
    "max_category_name = np.max(train.category_name.max())+1\n",
    "max_parent_category_name = np.max(train.parent_category_name.max())+1\n",
    "max_param_1 = np.max(train.param_1.max())+1\n",
    "max_param123 = np.max(train.param123.max())+1\n",
    "    \n",
    "print(\"Train Test Split\")\n",
    "x_train_f, x_valid_f, y_train_f, y_valid_f = train_test_split(train, y_train, train_size=0.95, random_state=233)\n",
    "print(x_train_f.shape, x_valid_f.shape)\n",
    "print(y_train_f.shape, y_valid_f.shape)\n",
    "\n",
    "del train, y_train\n",
    "gc.collect()\n",
    "\n",
    "X_train = get_keras_data(x_train_f)\n",
    "X_valid = get_keras_data(x_valid_f)\n",
    "del x_train_f, x_valid_f\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748125, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# EMBEDDINGS COMBINATION \n",
    "# FASTTEXT\n",
    "\n",
    "EMBEDDING_DIM1 = 300\n",
    "EMBEDDING_FILE1 = './fasttest/cc.ru.300.vec'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index1 = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE1, encoding='utf-8'))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "EMBEDDING_DIM1 = 300# this is from the pretrained vectors\n",
    "embedding_matrix1 = np.zeros((vocab_size, EMBEDDING_DIM1))\n",
    "print(embedding_matrix1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281646 466478 466478 281646\n",
      "(748125, 300)\n",
      " FAST TEXT DONE\n",
      "Train on 1428252 samples, validate on 75172 samples\n",
      "Epoch 1/1\n",
      "1428252/1428252 [==============================] - 40691s 28ms/step - loss: 0.2308 - root_mean_squared_error: 0.2308 - val_loss: 0.2270 - val_root_mean_squared_error: 0.2270\n",
      "0    False\n",
      "dtype: bool\n",
      "0    False\n",
      "dtype: bool\n",
      " RMSE for VALIDATION SET: 0.2270863845239294\n",
      "Train on 1428252 samples, validate on 75172 samples\n",
      "Epoch 1/1\n",
      "1428252/1428252 [==============================] - 3464s 2ms/step - loss: 0.2198 - root_mean_squared_error: 0.2198 - val_loss: 0.2259 - val_root_mean_squared_error: 0.2259\n",
      "0    False\n",
      "dtype: bool\n",
      "0    False\n",
      "dtype: bool\n",
      " RMSE for VALIDATION SET: 0.22592986637300555\n",
      "Train on 1428252 samples, validate on 75172 samples\n",
      "Epoch 1/1\n",
      "1428252/1428252 [==============================] - 8936s 6ms/step - loss: 0.2110 - root_mean_squared_error: 0.2110 - val_loss: 0.2289 - val_root_mean_squared_error: 0.2289\n",
      "0    False\n",
      "dtype: bool\n",
      "0    False\n",
      "dtype: bool\n",
      " RMSE for VALIDATION SET: 0.22888269118965235\n",
      "Finished Fitting the model\n",
      "item_id                  object\n",
      "user_id                  object\n",
      "region                   object\n",
      "city                     object\n",
      "parent_category_name     object\n",
      "category_name            object\n",
      "param_1                  object\n",
      "param_2                  object\n",
      "param_3                  object\n",
      "title                    object\n",
      "description              object\n",
      "price                   float64\n",
      "item_seq_number           int64\n",
      "activation_date          object\n",
      "user_type                object\n",
      "image                    object\n",
      "image_top_1             float64\n",
      "dtype: object\n",
      " Chunk number is 1\n",
      "Filling Missing Values.....\n",
      "Casting data types to type Category.......\n",
      "Creating New Feature.....\n",
      "PreProcessing Function completed.\n",
      "Transform done for test\n",
      "Time taken for Sequence Tokens is31.63163137435913\n",
      "Transform on test function completed.\n",
      "Data ready for Vectorization\n",
      "250000/250000 [==============================] - 273s 1ms/step\n",
      "(250000, 1)\n",
      "RNN Prediction is done\n",
      "(250000, 1)\n",
      "(250000,)\n",
      "(250000,)\n",
      "item_id                  object\n",
      "user_id                  object\n",
      "region                   object\n",
      "city                     object\n",
      "parent_category_name     object\n",
      "category_name            object\n",
      "param_1                  object\n",
      "param_2                  object\n",
      "param_3                  object\n",
      "title                    object\n",
      "description              object\n",
      "price                   float64\n",
      "item_seq_number           int64\n",
      "activation_date          object\n",
      "user_type                object\n",
      "image                    object\n",
      "image_top_1             float64\n",
      "dtype: object\n",
      " Chunk number is 2\n",
      "Filling Missing Values.....\n",
      "Casting data types to type Category.......\n",
      "Creating New Feature.....\n",
      "PreProcessing Function completed.\n",
      "Transform done for test\n",
      "Time taken for Sequence Tokens is27.261043071746826\n",
      "Transform on test function completed.\n",
      "Data ready for Vectorization\n",
      "250000/250000 [==============================] - 310s 1ms/step\n",
      "(250000, 1)\n",
      "RNN Prediction is done\n",
      "(250000, 1)\n",
      "(500000,)\n",
      "(500000,)\n",
      "item_id                  object\n",
      "user_id                  object\n",
      "region                   object\n",
      "city                     object\n",
      "parent_category_name     object\n",
      "category_name            object\n",
      "param_1                  object\n",
      "param_2                  object\n",
      "param_3                  object\n",
      "title                    object\n",
      "description              object\n",
      "price                   float64\n",
      "item_seq_number           int64\n",
      "activation_date          object\n",
      "user_type                object\n",
      "image                    object\n",
      "image_top_1             float64\n",
      "dtype: object\n",
      " Chunk number is 3\n",
      "Filling Missing Values.....\n",
      "Casting data types to type Category.......\n",
      "Creating New Feature.....\n",
      "PreProcessing Function completed.\n",
      "Transform done for test\n",
      "Time taken for Sequence Tokens is1.3268446922302246\n",
      "Transform on test function completed.\n",
      "Data ready for Vectorization\n",
      "8438/8438 [==============================] - 9s 1ms/step\n",
      "(8438, 1)\n",
      "RNN Prediction is done\n",
      "(8438, 1)\n",
      "(508438,)\n",
      "(508438,)\n",
      "All chunks done\n",
      "Total time for Parallel Batch Prediction is 701.8857762813568\n",
      "Check Submission NOW!!!!!!!!@\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating Embedding matrix \n",
    "c = 0 \n",
    "c1 = 0 \n",
    "w_Y = []\n",
    "w_No = []\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embeddings_index1:\n",
    "        c +=1\n",
    "        embedding_vector = embeddings_index1[word]\n",
    "        w_Y.append(word)\n",
    "    else:\n",
    "        embedding_vector = None\n",
    "        w_No.append(word)\n",
    "        c1 +=1\n",
    "    if embedding_vector is not None:    \n",
    "        embedding_matrix1[i] = embedding_vector\n",
    "\n",
    "print(c,c1, len(w_No), len(w_Y))\n",
    "print(embedding_matrix1.shape)\n",
    "del embeddings_index1\n",
    "gc.collect()\n",
    "\n",
    "print(\" FAST TEXT DONE\")\n",
    "\n",
    "\n",
    "\n",
    "def RNN_model():\n",
    "\n",
    "    #Inputs\n",
    "    seq_title_description = Input(shape=[X_train[\"seq_title_description\"].shape[1]], name=\"seq_title_description\")\n",
    "    region = Input(shape=[1], name=\"region\")\n",
    "    city = Input(shape=[1], name=\"city\")\n",
    "    category_name = Input(shape=[1], name=\"category_name\")\n",
    "    parent_category_name = Input(shape=[1], name=\"parent_category_name\")\n",
    "    param_1 = Input(shape=[1], name=\"param_1\")\n",
    "    param123 = Input(shape=[1], name=\"param123\")\n",
    "    price = Input(shape=[1], name=\"price\")\n",
    "    item_seq_number = Input(shape = [1], name = 'item_seq_number')\n",
    "    \n",
    "    #Embeddings layers\n",
    "\n",
    "    emb_seq_title_description = Embedding(vocab_size, EMBEDDING_DIM1, weights = [embedding_matrix1], trainable = True)(seq_title_description)\n",
    "    emb_region = Embedding(max_region, 10)(region)\n",
    "    emb_city = Embedding(max_city, 10)(city)\n",
    "    emb_category_name = Embedding(max_category_name, 10)(category_name)\n",
    "    emb_parent_category_name = Embedding(max_parent_category_name, 10)(parent_category_name)\n",
    "    emb_param_1 = Embedding(max_param_1, 10)(param_1)\n",
    "    emb_param123 = Embedding(max_param123, 10)(param123)\n",
    "\n",
    "    rnn_layer1 = GRU(25) (emb_seq_title_description)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "          rnn_layer1\n",
    "        , Flatten() (emb_region)\n",
    "        , Flatten() (emb_city)\n",
    "        , Flatten() (emb_category_name)\n",
    "        , Flatten() (emb_parent_category_name)\n",
    "        , Flatten() (emb_param_1)\n",
    "        , Flatten() (emb_param123)\n",
    "        , price\n",
    "        , item_seq_number\n",
    "    ])\n",
    "    \n",
    "    main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n",
    "    main_l = Dropout(0.1)(Dense(64,activation='relu') (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1,activation=\"sigmoid\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([seq_title_description, region, city, category_name, parent_category_name, param_1, param123, price, item_seq_number ], output)\n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss= root_mean_squared_error,\n",
    "                  metrics = [root_mean_squared_error])\n",
    "    return model\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "\n",
    "    Rsum = np.sum((y - y_pred)**2)\n",
    "    n = y.shape[0]\n",
    "    RMSE = np.sqrt(Rsum/n)\n",
    "    return RMSE \n",
    "\n",
    "def eval_model(model):\n",
    "    val_preds = model.predict(X_valid)\n",
    "    y_pred = val_preds[:, 0]\n",
    "    \n",
    "    y_true = np.array(y_valid_f)\n",
    "    \n",
    "    yt = pd.DataFrame(y_true)\n",
    "    yp = pd.DataFrame(y_pred)\n",
    "    \n",
    "    print(yt.isnull().any())\n",
    "    print(yp.isnull().any())\n",
    "    \n",
    "    v_rmse = rmse(y_true, y_pred)\n",
    "    print(\" RMSE for VALIDATION SET: \"+str(v_rmse))\n",
    "    return v_rmse\n",
    "\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "epochs = 1\n",
    "BATCH_SIZE = 512 * 3\n",
    "steps = int(len(X_train['seq_title_description'])/BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.009, 0.0045\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "modelRNN = RNN_model()\n",
    "K.set_value(modelRNN.optimizer.lr, lr_init)\n",
    "K.set_value(modelRNN.optimizer.decay, lr_decay)\n",
    "\n",
    "del embedding_matrix1\n",
    "gc.collect()\n",
    "\n",
    "for i in range(3):\n",
    "    history = modelRNN.fit(X_train, y_train_f\n",
    "                    , epochs=epochs\n",
    "                    , batch_size= (BATCH_SIZE+(BATCH_SIZE*(i)))\n",
    "                    , validation_data = (X_valid, y_valid_f)\n",
    "                    , verbose=1\n",
    "                    )\n",
    "    # Evaluate RMSLE \n",
    "    v_rmse = eval_model(modelRNN)\n",
    "    \n",
    "print(\"Finished Fitting the model\")\n",
    "\n",
    "\n",
    "del X_train, y_train_f, X_valid, y_valid_f\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "import time\n",
    "t1 = time.time()\n",
    "def load_test():\n",
    "    for df in pd.read_csv('./inputs/test.csv', chunksize= 250000):\n",
    "        yield df\n",
    "\n",
    "item_ids = np.array([], dtype=np.int32)\n",
    "preds= np.array([], dtype=np.float32)\n",
    "\n",
    "i = 0 \n",
    "    \n",
    "for df in load_test():\n",
    "    \n",
    "    i +=1\n",
    "    print(df.dtypes)\n",
    "    item_id = df['item_id']\n",
    "    print(\" Chunk number is \"+str(i))\n",
    "    test = preprocess_dataset(df)\n",
    "    test = keras_test_transform(df)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    X_test = get_keras_data(test)\n",
    "    del test \n",
    "    gc.collect()\n",
    "    \n",
    "    preds1 = modelRNN.predict(X_test, batch_size = BATCH_SIZE, verbose = 1)\n",
    "    print(preds1.shape)\n",
    "    del X_test\n",
    "    gc.collect()\n",
    "    print(\"RNN Prediction is done\")\n",
    "\n",
    "    preds1 = preds1.reshape(-1,1)\n",
    "    #print(predsl.shape)\n",
    "    preds1 = np.clip(preds1, 0, 1)\n",
    "    print(preds1.shape)\n",
    "    item_ids = np.append(item_ids, item_id)\n",
    "    print(item_ids.shape)\n",
    "    preds = np.append(preds, preds1)\n",
    "    print(preds.shape)\n",
    "    \n",
    "print(\"All chunks done\")\n",
    "t2 = time.time()\n",
    "print(\"Total time for Parallel Batch Prediction is \"+str(t2-t1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "submission = pd.DataFrame( columns = ['item_id', 'deal_probability'])\n",
    "submission['item_id'] = item_ids\n",
    "submission['deal_probability'] = preds\n",
    "\n",
    "print(\"Check Submission\")\n",
    "submission.to_csv(\"Avito_rnn.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007472048596910419"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/wolfgangb33r/avito-prediction-xgboost-simple\n",
    "# Simple first attempt to predict the propability of demand\n",
    "# Not using the image info so far and only taking simple \n",
    "# categorical features into account\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import os.path\n",
    "import gc\n",
    "import random\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "def print_duration (start_time, msg):\n",
    "    print(\"[%d] %s\" % (int(time.time() - start_time), msg))\n",
    "    start_time = time.time()\n",
    "    return start_time\n",
    "    \n",
    "# quick way of calculating a numeric has for a string\n",
    "def n_hash(s):\n",
    "    random.seed(hash(s))\n",
    "    return random.random()\n",
    "\n",
    "# hash a complete column of a pandas dataframe    \n",
    "def hash_column (row, col):\n",
    "    if col in row:\n",
    "        return n_hash(row[col])\n",
    "    return n_hash('none')\n",
    "\n",
    "def runMain():\n",
    "    start_time = time.time()\n",
    "    # create a xgboost model\n",
    "    model = xgb.XGBRegressor(n_estimators=400, learning_rate=0.05, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7)\n",
    "    \n",
    "    # load the training data\n",
    "    train = pd.read_csv('./inputs/train.csv')\n",
    "    # calculate consistent numeric hashes for any categorical features \n",
    "    train['user_hash'] = train.apply (lambda row: hash_column (row, 'user_id'),axis=1)\n",
    "    train['region_hash'] = train.apply (lambda row: hash_column (row, 'region'),axis=1)\n",
    "    train['city_hash'] = train.apply (lambda row: hash_column (row, 'city'),axis=1)\n",
    "    train['parent_category_name_hash'] = train.apply (lambda row: hash_column (row, 'parent_category_name'),axis=1)\n",
    "    train['category_name_hash'] = train.apply (lambda row: hash_column (row, 'category_name'),axis=1)\n",
    "    train['user_type_hash'] = train.apply (lambda row: hash_column (row, 'user_type'),axis=1)\n",
    "    # for the beginning I use only the information if there is an image or not \n",
    "    train['image_exists'] = train['image'].isnull().astype(int)\n",
    "    # calc log for price to reduce effect of very large price differences\n",
    "    train['price'] = np.log(train['price'])\n",
    "    #print(train.groupby(['image_exists']).image_exists.count())\n",
    "    #print(train['image_exists'])\n",
    "    start_time = print_duration (start_time, \"Finished reading\")   \n",
    "\n",
    "    # start training\n",
    "    train_X = train.as_matrix(columns=['image_top_1', 'user_hash', 'price', 'region_hash', 'city_hash', 'parent_category_name_hash', 'category_name_hash', 'user_type_hash', 'image_exists'])\n",
    "    model.fit(train_X, train['deal_probability'])\n",
    "    \n",
    "    # read test data set\n",
    "    test = pd.read_csv('./inputs/test.csv')\n",
    "    test['user_hash'] = test.apply (lambda row: hash_column (row, 'user_id'),axis=1)\n",
    "    test['region_hash'] = test.apply (lambda row: hash_column (row, 'region'),axis=1)\n",
    "    test['city_hash'] = test.apply (lambda row: hash_column (row, 'city'),axis=1)\n",
    "    test['parent_category_name_hash'] = test.apply (lambda row: hash_column (row, 'parent_category_name'),axis=1)\n",
    "    test['category_name_hash'] = test.apply (lambda row: hash_column (row, 'category_name'),axis=1)\n",
    "    test['user_type_hash'] = test.apply (lambda row: hash_column (row, 'user_type'),axis=1)\n",
    "    test['image_exists'] = test['image'].isnull().astype(int)\n",
    "    test['price'] = np.log(test['price'])\n",
    "    test_X = test.as_matrix(columns=['image_top_1', 'user_hash', 'price', 'region_hash', 'city_hash', 'parent_category_name_hash', 'category_name_hash', 'user_type_hash', 'image_exists'])\n",
    "    start_time = print_duration (start_time, \"Finished training, start prediction\")   \n",
    "    # predict the propabilities for binary classes    \n",
    "    pred = model.predict(test_X)\n",
    "    return pred, start_time\n",
    "\n",
    "pred, start_time = runMain()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
